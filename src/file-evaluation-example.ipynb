{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35209b80129773d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using the file evaluation example\n",
    "\n",
    "Use this workbook to evaluate a single survey file at a time. Once you install dependencies and configure settings, you can run this workbook to perform the evaluation in four steps:\n",
    "\n",
    "1. **Initalize** to load credentials and configuration.\n",
    "2. **Read** the survey file to be evaluated.\n",
    "3. **Parse** the survey file to extract the survey modules, questions, and translations.\n",
    "4. **Evaluate** the extracted survey content.  \n",
    "\n",
    "In the process, this workbook will use the `evaluation_engine` module to conduct the evaluation and the `survey_parser` module to read and parse the survey file.\n",
    "\n",
    "## Preparing to run this workbook\n",
    "\n",
    "### Installing dependencies\n",
    "\n",
    "1. Install Python dependencies: `pip install -r requirements.txt`\n",
    "2. Install poppler: `brew install poppler` (macOS) or [see here for other platforms](https://pdf2image.readthedocs.io/en/latest/installation.html)\n",
    "3. Install tesseract: `brew install tesseract` (macOS) or [see here for other platforms](https://tesseract-ocr.github.io/tessdoc/Installation.html)\n",
    "4. Download an English pipeline for spaCy: `python -m spacy download en_core_web_sm`\n",
    "\n",
    "### Configuring settings\n",
    "\n",
    "This workbook begins by loading credentials and configuration from an `.ini` file stored in `~/.hbai/file-evaluation-example.ini`. The `~` in the path refers to the current user's home directory, and the `.ini` file contents are as follows:\n",
    "\n",
    "    [openai]\n",
    "    summarize-model=gpt-4o\n",
    "    summarize-model-max-tokens=4000\n",
    "    openai-api-key=sk-keyhere\n",
    "    azure-api-key=keyhere\n",
    "    azure-api-base=https://almitra-azure-1.openai.azure.com/\n",
    "    azure-api-version=2023-05-15\n",
    "\n",
    "    [parsing]\n",
    "    extraction-model=gpt-4o\n",
    "    splitter_chunk_size=3000\n",
    "    splitter_overlap_size=500\n",
    "\n",
    "    [evaluation]\n",
    "    evaluation-model=gpt-4o\n",
    "    tiktoken-model=gpt-4o\n",
    "    minimum-importance=\n",
    "    evaluation-context=Baseline survey on consumption, savings, and financial well-being\n",
    "    evaluation-locations=United States (English), Mexico (Spanish)\n",
    "    evaluation-extra-instructions=\n",
    "    evaluation-input-path=~/Files/evaluation/example_baseline_survey.docx\n",
    "    evaluation-output-path=~/Files/evaluation\n",
    "    \n",
    "    [langsmith]\n",
    "    langsmith-api-key=optional_key_here\n",
    "\n",
    "The easiest way to get started is to take the `file-evaluation-example.ini` in the root of this repository, copy it to `~/.hbai/file-evaluation-example.ini`, and customize as follows:\n",
    "\n",
    "1. Configure for OpenAI access:\n",
    "    1. If you want to use OpenAI directly: replace `openai-api-key` with your OpenAI API key.\n",
    "    2. If you want to use Azure's OpenAI service: replace `azure-api-key` with your Azure API key, and replace `azure-api-base` and `azure-api-version` with the base URL and version of the Azure API you want to use.\n",
    "2. Set the `evaluation-context` and `evaluation-locations` with short descriptions to help the AI system understand the relevant context and setting. For each location in `evaluation-locations`, include both the location and the language, so that the system knows which languages are used in which settings.  \n",
    "3. Set the `evaluation-input-path` to the path of the survey file you want to evaluate.\n",
    "4. Set the `evaluation-output-path` to the path of the directory where you want to write the evaluation results.\n",
    "\n",
    "## Running this workbook\n",
    "\n",
    "Once you've installed dependencies and configured settings, you can run this workbook to evaluate a survey file. Run each cell in order, and follow the instructions in the output to review the raw and parsed data before continuing. All results (including intermediate outputs and log files) will be written to the `evaluation-output-path` you configured in the `.ini` file."
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:40:12.931042Z",
     "start_time": "2024-07-25T12:40:11.625997Z"
    }
   },
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "# for convenience, auto-reload modules when they've changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# configure logging to output all messages to stdout, initialize logger\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# load credentials and other configuration from a local ini file\n",
    "inifile_location = os.path.expanduser(\"~/.hbai/file-evaluation-example.ini\")\n",
    "inifile = configparser.RawConfigParser()\n",
    "inifile.read(inifile_location)\n",
    "\n",
    "# load configuration\n",
    "openai_api_key = inifile.get(\"openai\", \"openai-api-key\")\n",
    "azure_api_key = inifile.get(\"openai\", \"azure-api-key\")\n",
    "azure_api_base = inifile.get(\"openai\", \"azure-api-base\")\n",
    "azure_api_version = inifile.get(\"openai\", \"azure-api-version\")\n",
    "if openai_api_key and not azure_api_key:\n",
    "    api_provider = \"openai\"\n",
    "else:\n",
    "    api_provider = \"azure\"\n",
    "max_combined_tokens = int(inifile.get(\"openai\", \"summarize-model-max-tokens\"))\n",
    "summarize_model = inifile.get(\"openai\", \"summarize-model\")\n",
    "parsing_model = inifile.get(\"parsing\", \"extraction-model\")\n",
    "splitter_chunk_size = int(inifile.get(\"parsing\", \"splitter_chunk_size\"))\n",
    "splitter_overlap_size = int(inifile.get(\"parsing\", \"splitter_overlap_size\"))\n",
    "gpt_model = inifile.get(\"evaluation\", \"evaluation-model\")\n",
    "tiktoken_model = inifile.get(\"evaluation\", \"tiktoken-model\")\n",
    "minimum_importance = inifile.get(\"evaluation\", \"minimum-importance\")\n",
    "if not minimum_importance:\n",
    "    minimum_importance = 0\n",
    "evaluation_context = inifile.get(\"evaluation\", \"evaluation-context\")\n",
    "if not evaluation_context:\n",
    "    evaluation_context = \"Unknown\"\n",
    "evaluation_locations = inifile.get(\"evaluation\", \"evaluation-locations\")\n",
    "if not evaluation_locations:\n",
    "    evaluation_locations = \"Unknown\"\n",
    "evaluation_extra_instructions = inifile.get(\"evaluation\", \"evaluation-extra-instructions\")\n",
    "if not evaluation_extra_instructions:\n",
    "    evaluation_extra_instructions = \"\"\n",
    "evaluation_input_path = inifile.get(\"evaluation\", \"evaluation-input-path\")\n",
    "evaluation_output_path = inifile.get(\"evaluation\", \"evaluation-output-path\")\n",
    "langsmith_api_key = inifile.get(\"langsmith\", \"langsmith-api-key\")\n",
    "\n",
    "# support LangSmith for debugging (optional)\n",
    "if langsmith_api_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"survey-eval\"\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "aa7fb238d10caf83",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Reading the survey file\n",
    "\n",
    "The first step is to read the survey file to be evaluated. This step uses the `survey_parser` module to read the survey file and extract the raw text content of each page. The raw text content is then written to `raw_data.txt` in the `evaluation-output-path` you configured in the `.ini` file. Review the raw text content to ensure that it looks reasonable before continuing.\n",
    "\n",
    "## Structured data from REDCap and XLSForm (SurveyCTO, ODK, etc.)\n",
    "\n",
    "The `survey_parser` module can read structured data directly from REDCap and XLSForm files (used by SurveyCTO, ODK, and other platforms). In those cases, the data written to `raw_data.txt` will be JSON data, and parsing will be quick and painless (as no additional work will be required to parse its structure).\n",
    "\n",
    "## Beware garbage-in-garbage-out\n",
    "\n",
    "Depending on the format of your survey file, the `survey_parser` module might do a poor job reading the content. In that case, it's not worth continuing to parse and then evaluate the survey, because you'll only get garbage out. If the raw text content looks bad, you should either fix the survey file or use a different survey file.\n",
    "\n",
    "## Fixing the survey file\n",
    "\n",
    "For most unstructured survey formats, the closer you get to a simple Word file with minimal formatting, the easier it will be for the `survey_parser` module to read the content.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "id": "c96bae86b7ddbeb3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T12:40:12.979539Z",
     "start_time": "2024-07-25T12:40:12.932186Z"
    }
   },
   "source": [
    "# initialize for file reading and parsing\n",
    "from surveyeval.survey_parser import set_langchain_splits, generate_extractor_chain, extract_data\n",
    "set_langchain_splits(splitter_chunk_size, splitter_overlap_size)\n",
    "extractor_chain = generate_extractor_chain(parsing_model, azure_api_base, azure_api_key, azure_api_version)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "id": "29c30724b1e5625e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T12:40:13.050980Z",
     "start_time": "2024-07-25T12:40:12.980527Z"
    }
   },
   "source": [
    "# get data from URL\n",
    "from surveyeval.survey_parser import get_data_from_url\n",
    "import json\n",
    "raw_data = get_data_from_url(os.path.expanduser(evaluation_input_path))\n",
    "# output raw data to file in output path\n",
    "raw_data_output_path = os.path.join(os.path.expanduser(evaluation_output_path), \"raw_data.txt\")\n",
    "with open(raw_data_output_path, 'w', encoding='utf-8') as f:\n",
    "    if isinstance(raw_data, dict):\n",
    "        json.dump(raw_data, f, indent=2)\n",
    "\n",
    "        print()\n",
    "        print(f\"JSON data written to {raw_data_output_path}. Because file was read as structured data to begin with, the parsing step will be quick and easy.\")\n",
    "    else:\n",
    "        # raw_data must be a list of strings\n",
    "        for item in raw_data:\n",
    "            f.write(f\"{item}\\n\")\n",
    "\n",
    "        print(f\"Raw data written to {raw_data_output_path}. Review it to ensure that it looks reasonable before continuing on to parsing.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw data written to /Users/crobert/Files/evaluation/raw_data.txt. Review it to ensure that it looks reasonable before continuing on to parsing.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "aa4e2838642c5462",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Parsing the survey file\n",
    "\n",
    "The next step is to parse the raw data into a series of modules and questions. If the original input file was read as structured data (e.g., an XLSForm file), this step is quick, easy, and generally error-free. Otherwise, the `survey_parser` module uses AI assistance to make sense of the raw data. The parsed survey data is then written to `parsed_data.json` in the `evaluation-output-path` you configured in the `.ini` file. Review the parsed content to ensure that it looks reasonable before continuing.\n",
    "\n",
    "The parsed data should look like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"MODULE 1\": {\n",
    "        \"module_name\": \"MODULE 1\",\n",
    "        \"module_title\": \"\",\n",
    "        \"module_intro\": \"\",\n",
    "        \"questions\": {\n",
    "            \"question_id_1\": [\n",
    "                {\n",
    "                    \"question\": \"Question 1 in English\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"label\": \"YES\",\n",
    "                            \"value\": \"1\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"label\": \"NO\",\n",
    "                            \"value\": \"2\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"Question 1 in Spanish\",\n",
    "                    \"language\": \"Spanish\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"label\": \"YES\",\n",
    "                            \"value\": \"1\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"label\": \"NO\",\n",
    "                            \"value\": \"2\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                }\n",
    "            ],\n",
    "            \"question_id_2\": [\n",
    "                {\n",
    "                    \"question\": \"Question 2 in English\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"options\": [],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"Question 2 in Spanish\",\n",
    "                    \"language\": \"Spanish\",\n",
    "                    \"options\": [],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"MODULE 2\": {\n",
    "        \"module_name\": \"MODULE 2\",\n",
    "        \"module_title\": \"\",\n",
    "        \"module_intro\": \"\",\n",
    "        \"questions\": {\n",
    "            \"question_id_3\": [\n",
    "                {\n",
    "                    \"question\": \"Question 3 in English\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"label\": \"YES\",\n",
    "                            \"value\": \"1\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"label\": \"NO\",\n",
    "                            \"value\": \"2\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"Question 3 in Spanish\",\n",
    "                    \"language\": \"Spanish\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"label\": \"YES\",\n",
    "                            \"value\": \"1\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"label\": \"NO\",\n",
    "                            \"value\": \"2\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                }\n",
    "            ],\n",
    "            \"question_id_4\": [\n",
    "                {\n",
    "                    \"question\": \"Question 4 in English\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"options\": [],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"Question 4 in Spanish\",\n",
    "                    \"language\": \"Spanish\",\n",
    "                    \"options\": [],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Beware garbage-in-garbage-out\n",
    "\n",
    "Depending on the format of your survey file, the `survey_parser` module might do a poor job parsing the content. In that case, it's not worth continuing to evaluate the survey, because you'll only get garbage out. If the parsed content looks bad, you should either fix the survey file or use a different survey file. You can also try to adjust the `splitter_chunk_size` and `splitter_overlap_size` settings in the `.ini` file to see if that helps.\n",
    "\n",
    "## Fixing the survey file\n",
    "\n",
    "The closer you get to a simple Word file with minimal formatting, the easier it will be for the `survey_parser` module to parse the content. Try to:\n",
    " \n",
    "1. Make module headings obvious\n",
    "2. Label each question with a unique number or ID\n",
    "3. Keep different translations for each question together, with the same unique number or ID  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "id": "f2e5eca78ea7900f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:40:29.643960Z",
     "start_time": "2024-07-25T12:40:13.052377Z"
    }
   },
   "source": [
    "# PARSE FILE FOR EVALUATION\n",
    "\n",
    "# output path to file we'll process\n",
    "import json\n",
    "print(evaluation_input_path)\n",
    "\n",
    "# parse file\n",
    "extraction_results = await extract_data(extractor_chain, os.path.expanduser(evaluation_input_path))\n",
    "data = extraction_results[\"modules\"]\n",
    "errors = extraction_results[\"errors\"]\n",
    "\n",
    "# output errors to console\n",
    "if errors:\n",
    "    print()\n",
    "    print(f\"{len(errors)} error(s) occurred during parsing:\")\n",
    "    for error in errors:\n",
    "        print()\n",
    "        print(error)\n",
    "\n",
    "# output parsed data to file so that the parsing step can itself be evaluated\n",
    "parsed_data_output_path = os.path.join(os.path.expanduser(evaluation_output_path), \"parsed_data.json\")\n",
    "with open(parsed_data_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print()\n",
    "print(f\"Parsed data written to {parsed_data_output_path}. Review it to ensure that it looks reasonable before continuing.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/Files/evaluation/SADHS_Small.docx\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: Tokens consumed:: 5772\n",
      "INFO:   Prompt tokens: 5046\n",
      "INFO:   Completion tokens: 726\n",
      "INFO: Successful Requests: 1\n",
      "INFO: Cost: $0.0\n",
      "INFO: * Module: FACTORS\n",
      "INFO: * Module: HISTORY1\n",
      "\n",
      "Parsed data written to /Users/crobert/Files/evaluation/parsed_data.json. Review it to ensure that it looks reasonable before continuing.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "a43ea2abe65e8e76",
   "metadata": {},
   "source": [
    "## Optional: Outputting parsed data to XLSForm\n",
    "\n",
    "If you want to output the parsed data to an XLSForm file, you can use the `output_parsed_data_to_xlsform` function from the `survey_parser` module. This function will write the parsed data to an XLSForm file that you can use to create a survey in a tool like SurveyCTO or ODK."
   ]
  },
  {
   "cell_type": "code",
   "id": "448b1438c9691caa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:40:29.789625Z",
     "start_time": "2024-07-25T12:40:29.644692Z"
    }
   },
   "source": [
    "# OUTPUT PARSED DATA TO XLSFORM\n",
    "\n",
    "from surveyeval.survey_parser import output_parsed_data_to_xlsform\n",
    "import os\n",
    "import re\n",
    "\n",
    "# construct filename for XLSForm output\n",
    "input_filename = os.path.basename(evaluation_input_path)\n",
    "input_filename_without_ext = os.path.splitext(input_filename)[0]\n",
    "new_filename = f\"{input_filename_without_ext}_xlsform.xlsx\"\n",
    "xlsform_output_path = os.path.join(os.path.expanduser(evaluation_output_path), new_filename)\n",
    "\n",
    "# construct a safe form ID from the input filename\n",
    "safe_form_id = re.sub(r'\\W+', '_', input_filename_without_ext).lower()\n",
    "# if it doesn't begin with a letter, put a 'f_' on the front\n",
    "if not safe_form_id[0].isalpha():\n",
    "    safe_form_id = 'f_' + safe_form_id\n",
    "\n",
    "# output parsed data to XLSForm file\n",
    "output_parsed_data_to_xlsform(data=data, form_id=safe_form_id, form_title=\"Automated form output\", output_file=xlsform_output_path)\n",
    "\n",
    "print(f\"XLSForm written to {xlsform_output_path}.\")"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XLSForm written to /Users/crobert/Files/evaluation/SADHS_Small_xlsform.xlsx.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6696ed5810240743",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Running the evaluation\n",
    "\n",
    "The final step is to run the evaluation. This step uses the `evaluation_engine` module to evaluate the parsed survey data. The evaluation results are then written to `aggregated_results.txt` in the `evaluation-output-path` you configured in the `.ini` file, along with a `logs.txt` file containing a detailed log of the evaluation process.\n",
    "\n",
    "The evaluation process has been parallelized to run more quickly, but it can still take a long time. You can monitor progress by watching the debug output in the console. If you want to stop the evaluation process, you can interrupt the kernel in your Jupyter notebook or press Ctrl+C in the console.\n",
    "\n",
    "Once you see the results, you might want to adjust the evaluation. If so, you can use the `extra_evaluation_instructions` setting in the `.ini` file to provide additional instructions to the evaluation lenses. For example:\n",
    "\n",
    "    evaluation-extra-instructions=* When making recommendations, use conversational language and phrasing \n",
    "     appropriate to informal household interviews.\n",
    "     \n",
    "     * When evaluating language and phrasing for bias, stereotypes, and prejudice, don't be more sensitive than \n",
    "     is appropriate in the survey locations considered (e.g., don't impose Western values in non-Western settings).\n",
    "\n",
    "Note that if you have more than one line of extra instructions, each line from the second one on should be indented by one space.\n",
    "\n",
    "Finally, if there are just too many results for you, you can set `minimum_importance` in your `.ini` file to anywhere from 2 to 5 to filter out less important results. (The default is 0, which shows all results.) "
   ]
  },
  {
   "cell_type": "code",
   "id": "65fd55a95a8fc52f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T12:40:29.822731Z",
     "start_time": "2024-07-25T12:40:29.790567Z"
    }
   },
   "source": [
    "# initialize evaluation engine\n",
    "if 'evaluation_engine' in sys.modules:\n",
    "    # if it's already been loaded, force-reload to pick up any changes\n",
    "    del sys.modules['evaluation_engine']\n",
    "from surveyeval import EvaluationEngine, PhrasingEvaluationLens, ValidatedInstrumentEvaluationLens, BiasEvaluationLens, TranslationEvaluationLens\n",
    "evaluation_engine = EvaluationEngine(\n",
    "    summarize_model=summarize_model,\n",
    "    summarize_provider=api_provider,\n",
    "    evaluation_model=gpt_model,\n",
    "    evaluation_provider=api_provider,\n",
    "    openai_api_key=openai_api_key,\n",
    "    azure_api_key=azure_api_key,\n",
    "    azure_api_base=azure_api_base,\n",
    "    azure_api_version=azure_api_version,\n",
    "    temperature=0,\n",
    "    tiktoken_model_name=tiktoken_model,\n",
    "    max_retries=3,\n",
    "    logger=logger,\n",
    "    extra_evaluation_instructions=evaluation_extra_instructions\n",
    ")\n",
    "\n",
    "# initialize evaluation lenses\n",
    "phrasing_lens = PhrasingEvaluationLens(evaluation_engine)\n",
    "validated_instrument_lens = ValidatedInstrumentEvaluationLens(evaluation_engine)\n",
    "bias_lens = BiasEvaluationLens(evaluation_engine)\n",
    "translation_lens = TranslationEvaluationLens(evaluation_engine)\n",
    "\n",
    "print(\"Evaluation engine and lenses initialized. Execute next cell to conduct evaluation.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation engine and lenses initialized. Execute next cell to conduct evaluation.\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "dcea3cc5b0fe1bf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:41:21.506522Z",
     "start_time": "2024-07-25T12:40:29.823879Z"
    }
   },
   "source": [
    "# CONDUCT EVALUATION\n",
    "\n",
    "# initialize a list to store results data\n",
    "results_data = []\n",
    "\n",
    "# prepare for parallel processing of evaluation tasks\n",
    "import asyncio\n",
    "async def yield_chunked_tasks(task_list: list, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Yield successive chunk_size-sized chunks from tasks.\n",
    "\n",
    "    :param task_list: List of tasks to be chunked.\n",
    "    :type task_list: list\n",
    "    :param chunk_size: Size of each chunk.\n",
    "    :type chunk_size: int\n",
    "    :return: Yields chunks of tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, len(task_list), chunk_size):\n",
    "        yield task_list[i:i + chunk_size]\n",
    "\n",
    "async def process_task_chunks(task_list: list, chunk_size: int, delay: int):\n",
    "    \"\"\"\n",
    "    Process chunks of tasks with a specified delay.\n",
    "\n",
    "    :param task_list: List of tasks to process.\n",
    "    :type task_list: list\n",
    "    :param chunk_size: Size of each task chunk.\n",
    "    :type chunk_size: int\n",
    "    :param delay: Delay between processing each chunk (in seconds).\n",
    "    :type delay: int\n",
    "    \"\"\"\n",
    "\n",
    "    async def task_wrapper(task):\n",
    "        evaluation_type, text_excerpt, eval_lens, func = task\n",
    "        result = await func\n",
    "        return evaluation_type, text_excerpt, eval_lens, result\n",
    "    \n",
    "    chunk_index = 0\n",
    "    async for chunk in yield_chunked_tasks(task_list, chunk_size):\n",
    "        # process each task in the current chunk\n",
    "        wrapped_chunk = [task_wrapper(task) for task in chunk]\n",
    "        results_chunk = await asyncio.gather(*wrapped_chunk)\n",
    "        results_data.extend(results_chunk)\n",
    "        await asyncio.sleep(delay)\n",
    "        chunk_index += 1\n",
    "\n",
    "# initialize variables for tasks and results\n",
    "tasks = []\n",
    "translations = []\n",
    "\n",
    "# run through the parsed questionnaire data and generate list of async review tasks\n",
    "k = 0\n",
    "for module_key, module in data.items():\n",
    "    # run through all questions in the module, assembling questions and translations for review\n",
    "    module_questions = []\n",
    "    full_module_text = f\"{module_key}\\n\\n\"\n",
    "    if module['module_intro']:\n",
    "        full_module_text += f\"{module['module_intro']}\\n\\n\"\n",
    "    for question in module['questions'].values():\n",
    "        # consider each translation of the question\n",
    "        languages = []\n",
    "        translations = []\n",
    "        for translation in question:\n",
    "            option_labels = ' | '.join(option['label'] for option in translation['options'])\n",
    "            full_question = f\"Question: {translation['question']}\\nOptions: {option_labels}\\nInstructions: {translation['instructions']}\\n\"\n",
    "            \n",
    "            # conduct bias review on each translation independently\n",
    "            tasks.append((\"bias\", full_question, bias_lens, bias_lens.a_evaluate(\n",
    "                survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_question)))\n",
    "            \n",
    "            # save first language only for full phrasing and module-level review\n",
    "            if not languages:\n",
    "                module_questions.append(full_question)\n",
    "                full_module_text += full_question\n",
    "                full_module_text += \"\\n\"\n",
    "                \n",
    "            # remember this translation for translation review\n",
    "            languages.append(translation['language'])\n",
    "            translations.append(full_question)\n",
    "        \n",
    "        # conduct translation review whenever there are multiple translations\n",
    "        if len(languages) > 1:\n",
    "            primary_language_text = \"\"\n",
    "            for idx, language in enumerate(languages):\n",
    "                if not language:\n",
    "                    language = \"Unknown-language\"\n",
    "                \n",
    "                if idx == 0:\n",
    "                    primary_language_text = f\"Primary language ({language}):\\n\\n{translations[idx]}\\n\\n\"\n",
    "                else:\n",
    "                    questions_by_language = primary_language_text + f\"Translated language ({language}):\\n\\n{translations[idx]}\\n\\n\"\n",
    "                    \n",
    "                    # conduct translation review in pairwise fashion, with each translation against the first\n",
    "                    # (primary) language\n",
    "                    tasks.append((\"translation\", questions_by_language, translation_lens, translation_lens.a_evaluate(survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=questions_by_language)))\n",
    "\n",
    "    # execute phrasing evaluation on each module question (first language only)\n",
    "    for module_question in module_questions:\n",
    "        tasks.append((\"phrasing\", module_question, phrasing_lens, phrasing_lens.a_evaluate(survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_module_text, survey_question=module_question)))\n",
    "\n",
    "    # execute module-level evaluation lenses\n",
    "    tasks.append((\"validated_instrument\", full_module_text, validated_instrument_lens, validated_instrument_lens.a_evaluate(survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_module_text)))\n",
    "\n",
    "# process tasks in chunks, with a delay between chunks\n",
    "await process_task_chunks(tasks, chunk_size=10, delay=5)\n",
    "\n",
    "print()\n",
    "print(f\"Completed evaluation of {len(results_data)} tasks. Execute next cell to output results.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Parent run 4f047e16-3780-46d4-86a8-f60e91cdacd7 not found for run 816a7bb1-099d-408b-8ae6-d47757bd00d7. Treating as a root run.\n",
      "WARNING: Parent run 7c37fbc6-f222-4245-81fc-73743b654966 not found for run 28716ee0-16f7-44c8-83d0-88ab45cff763. Treating as a root run.\n",
      "WARNING: Parent run 073dc34e-97e9-4f07-8dd3-50f8bfec5823 not found for run c02715c0-35f0-4fde-90c9-3c62efd05a63. Treating as a root run.\n",
      "WARNING: Parent run f166c4d3-9c94-4313-909f-0374473fd35c not found for run 3f40f3af-2bd3-4e44-8ddb-4bcddd87917c. Treating as a root run.\n",
      "WARNING: Parent run f1886238-844c-4ca1-8e8a-23f52320fc10 not found for run 3c0974a3-0bde-4dc0-a3e1-ce98b43b48b1. Treating as a root run.\n",
      "WARNING: Parent run 58343e37-2ad1-46b1-b76b-f4ae004e53af not found for run 94db2592-893f-49bf-aa4e-34c66a0bfbbb. Treating as a root run.\n",
      "WARNING: Parent run 40a389de-6e90-42aa-ac23-c64a82b1bfe2 not found for run f8deb6f0-134f-423d-8650-17f1e0de777f. Treating as a root run.\n",
      "WARNING: Parent run 93450fdf-53ea-489d-9d73-0d6776c7feaa not found for run 1d18b6c3-80ea-4f5d-a676-59ff277eda12. Treating as a root run.\n",
      "WARNING: Parent run 790a7757-0a1d-4852-b9a3-687457f8ebbb not found for run 4666989e-7517-4b69-97a1-535575208a90. Treating as a root run.\n",
      "WARNING: Parent run 7ea253e4-afea-4dee-b0e6-e8cd247465ed not found for run dc4197fb-1ea2-4822-9f22-3fdab912f6bf. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 838355a9-f8ca-433f-a1e1-8beed3fe180c not found for run 7f694265-5e2f-4014-9d27-0af679d4136c. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 90971ca6-98ae-450d-bf09-5178e97c00a3 not found for run 64ef9dfb-de23-466e-bc64-3d4b1dce307d. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 5df96102-dde4-477e-a550-1f1526808238 not found for run 11903217-42fb-423f-a71c-585a5e122a1d. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 4d5f011e-37c1-4540-acb2-4d06264952f9 not found for run a3401f2c-85a9-4be3-97c8-690307264e47. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run df0b2e68-669e-4048-baef-7e431bfb8ec8 not found for run a5597707-6a66-42d3-896f-ceeab0980ca9. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 8f4094e9-6f57-443e-98e3-4cf30449c02a not found for run 14c8a9a2-25de-4bbb-81a1-52fc599a3c73. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 7b698da8-4ef5-45b8-a3ce-2e47cf3afeda not found for run c95a970e-8cdb-482d-8d7c-6768181d86fb. Treating as a root run.\n",
      "WARNING: Parent run 56fd4933-e9ba-44b5-99e8-ad4af98ac9f1 not found for run 25ecf1ed-76bf-4a5a-ab80-a24510293dea. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run aace219b-d0b5-482e-b559-2fc0456b9985 not found for run 3e559b7d-0a13-44da-ae1c-ad012c408138. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run b6652f9e-66a7-4a64-b1ac-ee5861f80161 not found for run 0a9ea590-8d6f-4cef-b68f-c4fd841c7a13. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 313ed61e-bbbc-4ef6-93e3-90ccf344d1a4 not found for run 6bbaf18e-6e42-4b87-852a-55e940b618e3. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run a701204c-3e8d-48fe-b779-36197072273b not found for run 24c92465-858a-4ddc-8933-4c29f207f1ac. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run a66575a7-bcc7-49b4-a9f3-b31e81e2738e not found for run 1b940b57-dbf4-4d09-a161-ddafa2c77227. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run b8ba85d9-9493-4610-b1fd-be4eb86e080f not found for run a965da7f-66fc-4e33-a8a4-65c1a3ca7de2. Treating as a root run.\n",
      "WARNING: Parent run 18b0f75b-1732-4d47-9b0d-e8e954523154 not found for run a96c685f-c2b6-44d5-b9b0-443fe5514cb2. Treating as a root run.\n",
      "WARNING: Parent run 303ee229-4226-4f33-95a1-9a9b3468518f not found for run c0963efa-675c-44e0-a125-bba6540f5f9c. Treating as a root run.\n",
      "WARNING: Parent run 5e4029d7-fe72-42e6-a38a-535a946772de not found for run a516f7f1-b506-4be7-9559-96282b036012. Treating as a root run.\n",
      "WARNING: Parent run f7a4acd4-1abf-49c8-85d0-89f119d0f893 not found for run 9e841699-bd2d-4254-92bf-226db994020c. Treating as a root run.\n",
      "WARNING: Parent run ba3557e6-61ea-4d64-8ce6-d14570684743 not found for run f4e86b4a-6f4b-4006-9e59-b6cb99243496. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 250bb4d7-97ad-4e33-8926-3e500ba4ab02 not found for run b3d9f2bc-33be-4be9-9814-f6efcc5b940f. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run f3ae7e9f-d937-4245-abf6-5f5ba50b2cf0 not found for run af739842-117e-4f0c-b05f-319fdfd772ab. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run bf4994aa-5fd4-4f83-b21f-84d6ca1f2291 not found for run baa0dd95-1d36-4ba5-a4c6-7cfac6a1f060. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run dfa7ac31-9867-4b7a-a14c-8640bdc2d0b5 not found for run 742ae380-c96c-4eab-8d28-6c2360affe45. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 5d4c1147-41a0-40d3-ad6b-5229c3ece6ea not found for run 2fe6d1ee-2329-4cd7-8f74-341aee54cf64. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 0a874a90-74e6-4452-85a2-45ace3cb443b not found for run 979b8b18-b776-4881-a336-eb14c23ed1fe. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run f89f8326-9ae9-47fe-ba6f-2c442857f544 not found for run 232e3422-5e36-4bec-8857-bb4088b9b7bc. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "WARNING: Parent run 7561c029-7fcb-4c32-aa7b-7ef239386a03 not found for run e069e0d8-8ed5-4b76-aee0-8c1226557a5b. Treating as a root run.\n",
      "INFO: HTTP Request: POST https://hbai-openai-useast2.openai.azure.com//openai/deployments/gpt-4o/chat/completions?api-version=2023-05-15 \"HTTP/1.1 200 OK\"\n",
      "\n",
      "Completed evaluation of 16 tasks. Execute next cell to output results.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "9d70c900cb040ccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-25T12:41:21.550958Z",
     "start_time": "2024-07-25T12:41:21.507833Z"
    }
   },
   "source": [
    "# output results, including a detailed processing log\n",
    "logs = []\n",
    "results = []\n",
    "results_by_type = {\n",
    "    \"phrasing\": [],\n",
    "    \"bias\": [],\n",
    "    \"translation\": [],\n",
    "    \"validated_instrument\": []\n",
    "}\n",
    "\n",
    "def add_log(new_history):\n",
    "    for his in new_history:\n",
    "        logs.append(\"PROMPT:\")\n",
    "        logs.append(his[0])\n",
    "        logs.append(\"RESPONSE:\")\n",
    "        logs.append(his[1])\n",
    "\n",
    "for output in results_data:\n",
    "    eval_type = output[0]\n",
    "    excerpt = output[1]\n",
    "    lens = output[2]\n",
    "    result_dict = output[3]\n",
    "\n",
    "    # retrieve and format result\n",
    "    if result_dict[\"result\"] == \"success\":\n",
    "        formatted_result = lens.format_result(result=result_dict[\"response\"], minimum_importance=minimum_importance)\n",
    "    else:\n",
    "        formatted_result = f\"Error: {result_dict['error']}\"\n",
    "\n",
    "    # determine appropriate evaluation header\n",
    "    eval_headers = {\n",
    "        \"phrasing\": \"Phrasing evaluation for excerpt:\",\n",
    "        \"bias\": \"Bias evaluation for excerpt:\",\n",
    "        \"translation\": \"Translation evaluation for excerpt:\",\n",
    "        \"validated_instrument\": \"Module evaluation for excerpt:\"\n",
    "    }\n",
    "    eval_header = eval_headers.get(eval_type, \"UNKNOWN EVALUATION\")\n",
    "\n",
    "    # report result\n",
    "    if formatted_result is not None and formatted_result:\n",
    "        # report first for aggregate results output\n",
    "        results.append(eval_header)\n",
    "        results.append(excerpt.strip())\n",
    "        results.append(formatted_result.strip() + \"\\n\")\n",
    "        # report also for type-specific results output\n",
    "        type_results = results_by_type.get(eval_type, [])\n",
    "        type_results.append(\"Excerpt considered:\")\n",
    "        type_results.append(excerpt.strip())\n",
    "        type_results.append(formatted_result.strip() + \"\\n\")\n",
    "    \n",
    "    # log full exchange+result for transparency\n",
    "    logs.append(eval_header)\n",
    "    if result_dict[\"history\"]:\n",
    "        add_log(result_dict[\"history\"])\n",
    "    logs.append(formatted_result)\n",
    "\n",
    "# write results to files in evaluation_output_path\n",
    "print()\n",
    "logs_output_path = os.path.join(os.path.expanduser(evaluation_output_path), \"logs.txt\")\n",
    "with open(logs_output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n\\n'.join(logs))\n",
    "print(f\"Detailed logs written to {logs_output_path}.\")\n",
    "\n",
    "results_output_path = os.path.join(os.path.expanduser(evaluation_output_path), \"aggregated_results.txt\")\n",
    "with open(results_output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n\\n'.join(results))\n",
    "print(f\"All results written to {results_output_path}.\")\n",
    "\n",
    "for eval_type, eval_results in results_by_type.items():\n",
    "    if eval_results:\n",
    "        type_results_output_path = os.path.join(os.path.expanduser(evaluation_output_path), f\"aggregated_results_{eval_type}.txt\")\n",
    "        with open(type_results_output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n\\n'.join(eval_results))\n",
    "        print(f\"{eval_type} results written to {type_results_output_path}.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed logs written to /Users/crobert/Files/evaluation/logs.txt.\n",
      "All results written to /Users/crobert/Files/evaluation/aggregated_results.txt.\n",
      "phrasing results written to /Users/crobert/Files/evaluation/aggregated_results_phrasing.txt.\n",
      "bias results written to /Users/crobert/Files/evaluation/aggregated_results_bias.txt.\n",
      "validated_instrument results written to /Users/crobert/Files/evaluation/aggregated_results_validated_instrument.txt.\n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
