{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35209b80129773d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/higherbar-ai/survey-eval/blob/main/src/file-evaluation-example.ipynb\" target=\"_parent\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "# Using this file evaluation example\n",
    "\n",
    "This notebook will use [the `surveyeval` package](https://github.com/higherbar-ai/survey-eval) to evaluate a single survey file at a time. It is broken down into the following steps:\n",
    "\n",
    "1. **Set up** the runtime environment (you only have to do this once).\n",
    "2. **Initalize** to load credentials and configuration.\n",
    "3. **Prompt** for your survey file and contextual details.\n",
    "4. **Read** the survey file to be evaluated.\n",
    "5. **Parse** the survey file to extract the survey modules, questions, and translations.\n",
    "6. **Evaluate** the extracted survey content.\n",
    "7. **Output** the evaluation results.\n",
    "\n",
    "The notebook uses the `surveyeval` package's `survey_parser` module to read and parse the survey file and the `evaluation_engine` module to conduct the evaluation. If you're running this for the first time, you should run each code cell one at a time so that you can verify the results at each stage.\n",
    "\n",
    "See [the survey-eval GitHub repo](https://github.com/higherbar-ai/survey-eval) for more details.\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This notebook requires different settings depending on which AI service providers you want to use. If you're running in Google Colab, you configure these settings as \"secrets\"; just click the key icon in the left sidebar (and, once you create a secret, be sure to click the toggle to give the notebook access to the secret). If you're running this notebook in a different environment, you can set these settings in a `.env` file; the first time you run, it will write out a template `.env` file for you to fill in and direct you to its location.\n",
    "\n",
    "Following are the settings, regardless of the environment.\n",
    "\n",
    "### OpenAI (direct)\n",
    "\n",
    "To use OpenAI directly:\n",
    "\n",
    "* `openai_api_key` - your OpenAI API key (get one from [the OpenAI API key page](https://platform.openai.com/api-keys), and be sure to fund your platform account with at least $5 to allow GPT-4o model access)\n",
    "* `openai_model` (optional) - the model to use (defaults to `gpt-4o`)\n",
    "\n",
    "### OpenAI (via Microsoft Azure)\n",
    "\n",
    "To use OpenAI via Microsoft Azure:\n",
    "\n",
    "* `azure_api_key` - your Azure API key\n",
    "* `azure_api_base` - the base URL for the Azure API\n",
    "* `azure_api_engine` - the engine to use (a.k.a. the \"deployment\")\n",
    "* `azure_api_version` - the API version to use\n",
    "\n",
    "### Anthropic (direct)\n",
    "\n",
    "To use Anthropic directly:\n",
    "\n",
    "* `anthropic_api_key` - your Anthropic API key\n",
    "* `anthropic_model` - the model to use\n",
    "\n",
    "### LangSmith (for tracing)\n",
    "\n",
    "Optionally, you can add [LangSmith tracing](https://langchain.com/langsmith):\n",
    "\n",
    "* `langsmith_api_key` - your LangSmith API key\n",
    "\n",
    "## Your survey file\n",
    "\n",
    "The notebook will prompt you to select or upload a survey file. The simpler the file's formatting, the better the results will be.\n",
    "\n",
    "## Setting up the runtime environment\n",
    "\n",
    "This next code block installs all necessary Python and system packages into the current environment.\n",
    "\n",
    "**If you're running in Google Colab and it prompts you to restart the notebook in the middle of the installation steps, just click CANCEL.**"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# install Google Colab Support and surveyeval package\n",
    "%pip install colab-or-not surveyeval\n",
    "\n",
    "# download NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt', force=True)\n",
    "\n",
    "# set up our notebook environment (including LibreOffice)\n",
    "from colab_or_not import NotebookBridge\n",
    "notebook_env = NotebookBridge(\n",
    "    system_packages=[\"libreoffice\"],\n",
    "    config_path=\"~/.hbai/surveyeval.env\",\n",
    "    config_template={\n",
    "        \"openai_api_key\": \"\",\n",
    "        \"openai_model\": \"\",\n",
    "        \"azure_api_key\": \"\",\n",
    "        \"azure_api_base\": \"\",\n",
    "        \"azure_api_engine\": \"\",\n",
    "        \"azure_api_version\": \"\",\n",
    "        \"anthropic_api_key\": \"\",\n",
    "        \"anthropic_model\": \"\",\n",
    "        \"langsmith_api_key\": \"\",\n",
    "    }\n",
    ")\n",
    "notebook_env.setup_environment()"
   ],
   "id": "6990a612c8b314a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Initializing for survey evaluation\n",
    "\n",
    "The next code block initializes the notebook by loading settings and initializing for survey evaluation."
   ],
   "id": "b41ebd23d949fbf"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "# read all supported secrets\n",
    "openai_api_key = notebook_env.get_setting('openai_api_key')\n",
    "openai_model = notebook_env.get_setting('openai_model', 'gpt-4o')\n",
    "azure_api_key = notebook_env.get_setting('azure_api_key')\n",
    "azure_api_base = notebook_env.get_setting('azure_api_base')\n",
    "azure_api_engine = notebook_env.get_setting('azure_api_engine')\n",
    "azure_api_version = notebook_env.get_setting('azure_api_version')\n",
    "anthropic_api_key = notebook_env.get_setting(\"anthropic_api_key\")\n",
    "anthropic_model = notebook_env.get_setting(\"anthropic_model\")\n",
    "langsmith_api_key = notebook_env.get_setting('langsmith_api_key')\n",
    "\n",
    "# complain if we don't have the bare minimum to run\n",
    "if (not openai_api_key\n",
    "        and not (azure_api_key and azure_api_base and azure_api_engine and azure_api_version)\n",
    "        and not (anthropic_api_key and anthropic_model)):\n",
    "    raise Exception('We need settings set for OpenAI access (direct or via Azure) or for Anthropic access (direct). See the instructions above for more details.')\n",
    "\n",
    "# set LLM provider in preference order\n",
    "if azure_api_key:\n",
    "    api_provider = \"azure\"\n",
    "elif anthropic_api_key:\n",
    "    api_provider = \"anthropic\"\n",
    "else:\n",
    "    api_provider = \"openai\"\n",
    "\n",
    "# configure logging to output all messages to stdout, initialize logger\n",
    "import logging, sys\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# configure fixed output location\n",
    "output_dir = notebook_env.get_output_dir(not_colab_dir=\"~/surveyeval\", colab_subdir=\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompting for your survey file, plus contextual details\n",
    "\n",
    "This next code block prompts you to upload or select a single survey file for evaluation. If you don't have a survey file handy, you can use [this short excerpt from the DHS](https://github.com/higherbar-ai/ai-workflows/blob/main/resources/sample_dhs_questions.txt).\n",
    "\n",
    "It then prompts you for three parameters that allow you to contextualize the evaluation:\n",
    "\n",
    "1. **Evaluation context** - a brief description of the survey's purpose and target audience\n",
    "2. **Evaluation locations** - a list of locations where the survey will be conducted, including which languages are used in which settings (if applicable)\n",
    "3. **Extra evaluation instructions** - any additional instructions you want to provide to the evaluator\n",
    "\n",
    "You don't have to supply any of these details (you can just press *OK* on each prompt), but the more details you provide the more accurate the evaluation will be."
   ],
   "id": "b2c3b048d9a4a8f8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prompt for a single file and keep prompting till we get one\n",
    "input_path = \"\"\n",
    "while True:\n",
    "    # prompt for a survey file\n",
    "    survey_files = notebook_env.get_input_files(\"Survey file to evaluate\")\n",
    "\n",
    "    # complain if we didn't get just a single file\n",
    "    if len(survey_files) != 1:\n",
    "        print()\n",
    "        print('Please upload a single survey file.')\n",
    "        print()\n",
    "    else:\n",
    "        # fetch the path of the uploaded file\n",
    "        input_path = survey_files[0]\n",
    "\n",
    "        # break from loop\n",
    "        break\n",
    "\n",
    "# prompt for parameters\n",
    "evaluation_context = input(\"Evaluation context: \").strip()\n",
    "evaluation_locations = input(\"Evaluation locations and languages: \").strip()\n",
    "evaluation_extra_instructions = input(\"Extra evaluation instructions (if any): \").strip()\n",
    "\n",
    "# report results\n",
    "print(f\"Survey to evaluate: {input_path}\")\n",
    "print (f\"Evaluation context: {evaluation_context}\")\n",
    "print (f\"Evaluation locations: {evaluation_locations}\")\n",
    "print (f\"Extra evaluation instructions: {evaluation_extra_instructions}\")"
   ],
   "id": "b7dfd4a83fa80340",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa7fb238d10caf83",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Reading the survey file\n",
    "\n",
    "The first substantive step is to read the survey file to be evaluated. This step uses the `SurveyInterface` class in the `survey_parser` module to read the survey file and extract the raw content. That content is then written to the `evaluation-output-path` you configured in the `.ini` file, as either Markdown text in `raw_data.md` or JSON text in `raw_data.json`. Review the raw content to ensure that it looks reasonable before continuing.\n",
    "\n",
    "If you're running in Google Colab, files will be saved into the content folder. Find, view, and download them by clicking on the folder icon in the left sidebar.\n",
    "\n",
    "If you're running elsewhere, they will be saved into a `surveyeval` subdirectory created off of your user home directory.\n",
    "\n",
    "## Data from REDCap and XLSForm (SurveyCTO, ODK, etc.) saved as JSON\n",
    "\n",
    "The parser can read structured data directly from REDCap and XLSForm files (used by SurveyCTO, ODK, and other platforms). In those cases, the data will be written to `raw_data.json` in JSON format (rather than the default Markdown format), and parsing will be quick and painless (as no additional work will be required to parse its structure).\n",
    "\n",
    "## Other file formats saved as Markdown\n",
    "\n",
    "For some file formats (e.g., .pdf or .docx files), the parser will use the LLM to help convert documents into Markdown format — but that can be slow and a little bit costly (about $0.015/page). To disable this feature, set `use_llm` to `False` in the `read_survey_contents()` call in the code cell below.\n",
    "\n",
    "## Beware garbage-in-garbage-out\n",
    "\n",
    "Depending on the format of your survey file, the parser might do a poor job reading the content. In that case, it's not worth continuing to parse and then evaluate the survey, because you'll only get garbage out. If the raw text content looks bad, you should either fix the survey file or use a different survey file.\n",
    "\n",
    "### Fixing the survey file\n",
    "\n",
    "The closer you get to a simple Word file with minimal formatting, the easier it will be for the parser to parse the content. Try to:\n",
    "\n",
    "1. Use headings for each module\n",
    "2. Label each question with a unique number or ID\n",
    "3. Keep different translations for each question together, with the same unique number or ID"
   ]
  },
  {
   "cell_type": "code",
   "id": "29c30724b1e5625e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "import json, os\n",
    "\n",
    "# initialize for file reading and parsing\n",
    "from surveyeval.survey_parser import SurveyInterface\n",
    "if api_provider == \"openai\":\n",
    "    survey_interface = SurveyInterface(openai_api_key=openai_api_key, openai_model=openai_model, langsmith_api_key=langsmith_api_key)\n",
    "elif api_provider == \"azure\":\n",
    "    survey_interface = SurveyInterface(azure_api_key=azure_api_key, azure_api_engine=azure_api_engine, azure_api_base=azure_api_base, azure_api_version=azure_api_version, langsmith_api_key=langsmith_api_key)\n",
    "elif api_provider == \"anthropic\":\n",
    "    survey_interface = SurveyInterface(anthropic_api_key=anthropic_api_key, anthropic_model=anthropic_model, langsmith_api_key=langsmith_api_key)\n",
    "else:\n",
    "    raise Exception('Unsupported API provider.')\n",
    "\n",
    "# read survey file contents: set use_llm=False to disable LLM-based parsing\n",
    "survey_contents = survey_interface.read_survey_contents(os.path.expanduser(input_path), use_llm=True)\n",
    "\n",
    "# output read contents to file in output path\n",
    "already_parsed = isinstance(survey_contents, dict)\n",
    "raw_data_output_path = os.path.join(os.path.expanduser(output_dir), \"raw_data.json\" if already_parsed else \"raw_data.md\")\n",
    "with open(raw_data_output_path, 'w', encoding='utf-8') as f:\n",
    "    if already_parsed:\n",
    "        # write structured data to file as JSON\n",
    "        json.dump(survey_contents, f, indent=2)\n",
    "\n",
    "        print()\n",
    "        print(f\"JSON data written to {raw_data_output_path}. Because file was read as structured data to begin with, the parsing step will be quick and easy.\")\n",
    "    else:\n",
    "        # write raw data to file as Markdown text\n",
    "        f.write(survey_contents)\n",
    "\n",
    "        print()\n",
    "        print(f\"Raw data written to {raw_data_output_path}. Review it to ensure that it looks reasonable before continuing on to parsing.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aa4e2838642c5462",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Parsing the survey file\n",
    "\n",
    "The next step is to parse the raw data into a series of modules and questions. If the original input file was read as structured data (e.g., an XLSForm file), this step is quick, easy, and generally error-free. Otherwise, the parser uses AI assistance to make sense of the raw data. The parsed survey data is then written to `parsed_data.json` for your review. Be sure that it looks reasonable before continuing on to the next step.\n",
    "\n",
    "The parsed data should look something like this:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"MODULE 1\": {\n",
    "        \"module_name\": \"MODULE 1\",\n",
    "        \"module_title\": \"\",\n",
    "        \"module_intro\": \"\",\n",
    "        \"questions\": {\n",
    "            \"question_id_1\": [\n",
    "                {\n",
    "                    \"question\": \"Question 1 in English\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"label\": \"YES\",\n",
    "                            \"value\": \"1\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"label\": \"NO\",\n",
    "                            \"value\": \"2\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"Question 1 in Spanish\",\n",
    "                    \"language\": \"Spanish\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"label\": \"YES\",\n",
    "                            \"value\": \"1\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"label\": \"NO\",\n",
    "                            \"value\": \"2\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                }\n",
    "            ],\n",
    "            \"question_id_2\": [\n",
    "                {\n",
    "                    \"question\": \"Question 2 in English\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"options\": [],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"Question 2 in Spanish\",\n",
    "                    \"language\": \"Spanish\",\n",
    "                    \"options\": [],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"MODULE 2\": {\n",
    "        \"module_name\": \"MODULE 2\",\n",
    "        \"module_title\": \"\",\n",
    "        \"module_intro\": \"\",\n",
    "        \"questions\": {\n",
    "            \"question_id_3\": [\n",
    "                {\n",
    "                    \"question\": \"Question 3 in English\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"label\": \"YES\",\n",
    "                            \"value\": \"1\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"label\": \"NO\",\n",
    "                            \"value\": \"2\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"Question 3 in Spanish\",\n",
    "                    \"language\": \"Spanish\",\n",
    "                    \"options\": [\n",
    "                        {\n",
    "                            \"label\": \"YES\",\n",
    "                            \"value\": \"1\"\n",
    "                        },\n",
    "                        {\n",
    "                            \"label\": \"NO\",\n",
    "                            \"value\": \"2\"\n",
    "                        }\n",
    "                    ],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                }\n",
    "            ],\n",
    "            \"question_id_4\": [\n",
    "                {\n",
    "                    \"question\": \"Question 4 in English\",\n",
    "                    \"language\": \"English\",\n",
    "                    \"options\": [],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                },\n",
    "                {\n",
    "                    \"question\": \"Question 4 in Spanish\",\n",
    "                    \"language\": \"Spanish\",\n",
    "                    \"options\": [],\n",
    "                    \"instructions\": \"Instructions, if any\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "## Beware garbage-in-garbage-out\n",
    "\n",
    "Depending on the format of your survey file, the survey parser might do a poor job parsing the content. In that case, it's not worth continuing to evaluate the survey, because you'll only get garbage out. If the parsed content looks bad, you should either fix the survey file or use a different survey file.\n",
    "\n",
    "## Fixing the survey file\n",
    "\n",
    "The closer you get to a simple Word file with minimal formatting, the easier it will be for the parser to parse the content. Try to:\n",
    " \n",
    "1. Use headings for each module\n",
    "2. Label each question with a unique number or ID\n",
    "3. Keep different translations for each question together, with the same unique number or ID  \n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f2e5eca78ea7900f",
   "metadata": {},
   "source": [
    "# parse file\n",
    "data = survey_interface.parse_survey_contents(survey_contents=survey_contents, survey_context=evaluation_context)\n",
    "\n",
    "# output parsed data to file so that the parsing step can itself be evaluated\n",
    "parsed_data_output_path = os.path.join(os.path.expanduser(output_dir), \"parsed_data.json\")\n",
    "with open(parsed_data_output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print()\n",
    "print(f\"Parsed data written to {parsed_data_output_path}. Review it to ensure that it looks reasonable before continuing.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a43ea2abe65e8e76",
   "metadata": {},
   "source": [
    "## Optional: Outputting parsed data to XLSForm\n",
    "\n",
    "If you want to output the parsed data to an XLSForm file, you can use the `output_parsed_data_to_xlsform()` function. This function will write the parsed data to an XLSForm file that you can use to create a survey in a tool like SurveyCTO or ODK."
   ]
  },
  {
   "cell_type": "code",
   "id": "448b1438c9691caa",
   "metadata": {},
   "source": [
    "# OUTPUT PARSED DATA TO XLSFORM\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "# construct filename for XLSForm output\n",
    "input_filename = os.path.basename(input_path)\n",
    "input_filename_without_ext = os.path.splitext(input_filename)[0]\n",
    "new_filename = f\"{input_filename_without_ext}_xlsform.xlsx\"\n",
    "xlsform_output_path = os.path.join(os.path.expanduser(output_dir), new_filename)\n",
    "\n",
    "# construct a safe form ID from the input filename\n",
    "safe_form_id = re.sub(r'\\W+', '_', input_filename_without_ext).lower()\n",
    "# if it doesn't begin with a letter, put a 'f_' on the front\n",
    "if not safe_form_id[0].isalpha():\n",
    "    safe_form_id = 'f_' + safe_form_id\n",
    "\n",
    "# output parsed data to XLSForm file\n",
    "survey_interface.output_parsed_data_to_xlsform(data=data, form_id=safe_form_id, form_title=\"Automated form output\", output_file=xlsform_output_path)\n",
    "\n",
    "print(f\"XLSForm written to {xlsform_output_path}.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6696ed5810240743",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Running the evaluation\n",
    "\n",
    "The final step is to run the evaluation. This step uses the `evaluation_engine` module to evaluate the parsed survey data.\n",
    "\n",
    "The evaluation process has been parallelized to run more quickly, but it can still take a long time. You can monitor progress by watching the debug output in the console. If you want to stop the evaluation process, you can interrupt the kernel in your Jupyter notebook or press Ctrl+C in the console.\n",
    "\n",
    "Once you see the results, you might want to adjust the evaluation. If so, you can re-run the notebook and use the opportunity to provide extra instructions when prompted to do so. For example:\n",
    "\n",
    "    * When making recommendations, use conversational language and phrasing appropriate to\n",
    "      informal household interviews.\n",
    "     \n",
    "    * When evaluating language and phrasing for bias, stereotypes, and prejudice, don't be\n",
    "      more sensitive than is appropriate in the survey locations considered (e.g., don't\n",
    "      impose Western values in non-Western settings).\n",
    "\n",
    "## Rate limit errors\n",
    "\n",
    "If you encounter rate limit errors, you can change the following line to lower the ``chunk_size`` (which is the number of parallel requests made to the LLM):\n",
    "\n",
    "```python\n",
    "await process_task_chunks(tasks, chunk_size=5, delay=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "id": "65fd55a95a8fc52f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# initialize evaluation engine\n",
    "from surveyeval import EvaluationEngine, PhrasingEvaluationLens, ValidatedInstrumentEvaluationLens, BiasEvaluationLens, TranslationEvaluationLens\n",
    "\n",
    "# set model based on provider\n",
    "if api_provider == \"openai\":\n",
    "    eval_model = openai_model\n",
    "elif api_provider == \"azure\":\n",
    "    eval_model = azure_api_engine\n",
    "elif api_provider == \"anthropic\":\n",
    "    eval_model = anthropic_model\n",
    "else:\n",
    "    raise Exception('Unsupported LLM provider.')\n",
    "\n",
    "evaluation_engine = EvaluationEngine(\n",
    "    evaluation_model=eval_model,\n",
    "    evaluation_provider=api_provider,\n",
    "    openai_api_key=openai_api_key,\n",
    "    azure_api_key=azure_api_key,\n",
    "    azure_api_base=azure_api_base,\n",
    "    azure_api_version=azure_api_version,\n",
    "    anthropic_api_key=anthropic_api_key,\n",
    "    temperature=0,\n",
    "    max_retries=3,\n",
    "    logger=logger,\n",
    "    extra_evaluation_instructions=evaluation_extra_instructions,\n",
    "    langsmith_api_key=langsmith_api_key\n",
    ")\n",
    "\n",
    "# initialize evaluation lenses\n",
    "phrasing_lens = PhrasingEvaluationLens(evaluation_engine)\n",
    "validated_instrument_lens = ValidatedInstrumentEvaluationLens(evaluation_engine)\n",
    "bias_lens = BiasEvaluationLens(evaluation_engine)\n",
    "translation_lens = TranslationEvaluationLens(evaluation_engine)\n",
    "\n",
    "# initialize a list to store results data\n",
    "results_data = []\n",
    "\n",
    "# prepare for parallel processing of evaluation tasks\n",
    "import asyncio\n",
    "async def yield_chunked_tasks(task_list: list, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Yield successive chunk_size-sized chunks from tasks.\n",
    "\n",
    "    :param task_list: List of tasks to be chunked.\n",
    "    :type task_list: list\n",
    "    :param chunk_size: Size of each chunk.\n",
    "    :type chunk_size: int\n",
    "    :return: Yields chunks of tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(0, len(task_list), chunk_size):\n",
    "        yield task_list[i:i + chunk_size]\n",
    "\n",
    "async def process_task_chunks(task_list: list, chunk_size: int, delay: int):\n",
    "    \"\"\"\n",
    "    Process chunks of tasks with a specified delay.\n",
    "\n",
    "    :param task_list: List of tasks to process.\n",
    "    :type task_list: list\n",
    "    :param chunk_size: Size of each task chunk.\n",
    "    :type chunk_size: int\n",
    "    :param delay: Delay between processing each chunk (in seconds).\n",
    "    :type delay: int\n",
    "    \"\"\"\n",
    "\n",
    "    async def task_wrapper(task):\n",
    "        evaluation_type, text_excerpt, eval_lens, func = task\n",
    "        result = await func()\n",
    "        return evaluation_type, text_excerpt, eval_lens, result\n",
    "\n",
    "    chunk_index = 0\n",
    "    async for chunk in yield_chunked_tasks(task_list, chunk_size):\n",
    "        # process each task in the current chunk\n",
    "        wrapped_chunk = [task_wrapper(task) for task in chunk]\n",
    "        results_chunk = await asyncio.gather(*wrapped_chunk)\n",
    "        results_data.extend(results_chunk)\n",
    "        await asyncio.sleep(delay)\n",
    "        chunk_index += 1\n",
    "\n",
    "# initialize variables for tasks and results\n",
    "tasks = []\n",
    "translations = []\n",
    "\n",
    "# run through the parsed questionnaire data and generate list of async review tasks\n",
    "k = 0\n",
    "for module_key, module in data.items():\n",
    "    # run through all questions in the module, assembling questions and translations for review\n",
    "    module_questions = []\n",
    "    full_module_text = f\"{module_key}\\n\\n\"\n",
    "    if module['module_intro']:\n",
    "        full_module_text += f\"{module['module_intro']}\\n\\n\"\n",
    "    for question in module['questions'].values():\n",
    "        # consider each translation of the question\n",
    "        languages = []\n",
    "        translations = []\n",
    "        for translation in question:\n",
    "            option_labels = ' | '.join(option['label'] for option in translation['options'])\n",
    "            full_question = f\"Question: {translation['question']}\\nOptions: {option_labels}\\nInstructions: {translation['instructions']}\\n\"\n",
    "\n",
    "            # conduct bias review on each translation independently\n",
    "            tasks.append((\"bias\", full_question, bias_lens, lambda: bias_lens.a_evaluate(\n",
    "                survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_question)))\n",
    "\n",
    "            # save first language only for full phrasing and module-level review\n",
    "            if not languages:\n",
    "                module_questions.append(full_question)\n",
    "                full_module_text += full_question\n",
    "                full_module_text += \"\\n\"\n",
    "\n",
    "            # remember this translation for translation review\n",
    "            languages.append(translation['language'])\n",
    "            translations.append(full_question)\n",
    "\n",
    "        # conduct translation review whenever there are multiple translations\n",
    "        if len(languages) > 1:\n",
    "            primary_language_text = \"\"\n",
    "            for idx, language in enumerate(languages):\n",
    "                if not language:\n",
    "                    language = \"Unknown-language\"\n",
    "\n",
    "                if idx == 0:\n",
    "                    primary_language_text = f\"Primary language ({language}):\\n\\n{translations[idx]}\\n\\n\"\n",
    "                else:\n",
    "                    questions_by_language = primary_language_text + f\"Translated language ({language}):\\n\\n{translations[idx]}\\n\\n\"\n",
    "\n",
    "                    # conduct translation review in pairwise fashion, with each translation against the first\n",
    "                    # (primary) language\n",
    "                    tasks.append((\"translation\", questions_by_language, translation_lens, lambda: translation_lens.a_evaluate(survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=questions_by_language)))\n",
    "\n",
    "    # execute phrasing evaluation on each module question (first language only)\n",
    "    for module_question in module_questions:\n",
    "        tasks.append((\"phrasing\", module_question, phrasing_lens, lambda: phrasing_lens.a_evaluate(survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_module_text, survey_question=module_question)))\n",
    "\n",
    "    # execute module-level evaluation lenses\n",
    "    tasks.append((\"validated_instrument\", full_module_text, validated_instrument_lens, lambda: validated_instrument_lens.a_evaluate(survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_module_text)))\n",
    "\n",
    "# process tasks in chunks, with a delay between chunks\n",
    "await process_task_chunks(tasks, chunk_size=5, delay=1)\n",
    "\n",
    "print()\n",
    "print(f\"Completed evaluation of {len(results_data)} tasks. Execute next cell to output results.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Organizing and outputting the results\n",
    "\n",
    "This final code cell organizes and outputs final results to `aggregated_results.txt` as well as a `logs.txt` file containing a detailed log of the evaluation process.\n",
    "\n",
    "If you're running in Google Colab, files will be saved into the content folder. Find, view, and download them by clicking on the folder icon in the left sidebar.\n",
    "\n",
    "If you're running elsewhere, they will be saved into a `surveyeval` subdirectory created off of your user home directory."
   ],
   "id": "be84b71a8efcf94b"
  },
  {
   "cell_type": "code",
   "id": "9d70c900cb040ccf",
   "metadata": {},
   "source": [
    "# output results, including a detailed processing log\n",
    "logs = []\n",
    "results = []\n",
    "results_by_type = {\n",
    "    \"phrasing\": [],\n",
    "    \"bias\": [],\n",
    "    \"translation\": [],\n",
    "    \"validated_instrument\": []\n",
    "}\n",
    "\n",
    "def add_log(new_history):\n",
    "    for his in new_history:\n",
    "        logs.append(\"PROMPT:\")\n",
    "        logs.append(his[0])\n",
    "        logs.append(\"RESPONSE:\")\n",
    "        logs.append(his[1])\n",
    "\n",
    "for output in results_data:\n",
    "    eval_type = output[0]\n",
    "    excerpt = output[1]\n",
    "    lens = output[2]\n",
    "    result_dict = output[3]\n",
    "\n",
    "    # retrieve and format result\n",
    "    if result_dict[\"result\"] == \"success\":\n",
    "        formatted_result = lens.format_result(result=result_dict[\"response\"])\n",
    "    else:\n",
    "        formatted_result = f\"Error: {result_dict['error']}\"\n",
    "\n",
    "    # determine appropriate evaluation header\n",
    "    eval_headers = {\n",
    "        \"phrasing\": \"Phrasing evaluation for excerpt:\",\n",
    "        \"bias\": \"Bias evaluation for excerpt:\",\n",
    "        \"translation\": \"Translation evaluation for excerpt:\",\n",
    "        \"validated_instrument\": \"Module evaluation for excerpt:\"\n",
    "    }\n",
    "    eval_header = eval_headers.get(eval_type, \"UNKNOWN EVALUATION\")\n",
    "\n",
    "    # report result\n",
    "    if formatted_result is not None and formatted_result:\n",
    "        # report first for aggregate results output\n",
    "        results.append(eval_header)\n",
    "        results.append(excerpt.strip())\n",
    "        results.append(formatted_result.strip() + \"\\n\")\n",
    "        # report also for type-specific results output\n",
    "        type_results = results_by_type.get(eval_type, [])\n",
    "        type_results.append(\"Excerpt considered:\")\n",
    "        type_results.append(excerpt.strip())\n",
    "        type_results.append(formatted_result.strip() + \"\\n\")\n",
    "    \n",
    "    # log full exchange+result for transparency\n",
    "    logs.append(eval_header)\n",
    "    if result_dict[\"history\"]:\n",
    "        add_log(result_dict[\"history\"])\n",
    "    logs.append(formatted_result)\n",
    "\n",
    "# write results to files in evaluation_output_path\n",
    "print()\n",
    "logs_output_path = os.path.join(os.path.expanduser(output_dir), \"logs.txt\")\n",
    "with open(logs_output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n\\n'.join(logs))\n",
    "print(f\"Detailed logs written to {logs_output_path}.\")\n",
    "\n",
    "results_output_path = os.path.join(os.path.expanduser(output_dir), \"aggregated_results.txt\")\n",
    "with open(results_output_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n\\n'.join(results))\n",
    "print(f\"All results written to {results_output_path}.\")\n",
    "\n",
    "for eval_type, eval_results in results_by_type.items():\n",
    "    if eval_results:\n",
    "        type_results_output_path = os.path.join(os.path.expanduser(output_dir), f\"aggregated_results_{eval_type}.txt\")\n",
    "        with open(type_results_output_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('\\n\\n'.join(eval_results))\n",
    "        print(f\"{eval_type} results written to {type_results_output_path}.\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
