{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35209b80129773d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Using the file evaluation example\n",
    "\n",
    "Use this workbook to evaluate a single survey file at a time. Once you install dependencies and configure settings, you can run this workbook to perform the evaluation in four steps:\n",
    "\n",
    "1. **Initalize** to load credentials and configuration.\n",
    "2. **Read** the survey file to be evaluated.\n",
    "3. **Parse** the survey file to extract the survey modules, questions, and translations.\n",
    "4. **Evaluate** the extracted survey content.  \n",
    "\n",
    "In the process, this workbook will use the `evaluation_engine` module to conduct the evaluation and the `questionnaire_file_reader` and `questionnaire_file_parser` modules to read and parse the survey file.\n",
    "\n",
    "## Preparing to run this workbook\n",
    "\n",
    "### Installing dependencies\n",
    "\n",
    "1. Install Python dependencies: `pip install -r requirements.txt`\n",
    "2. Install poppler: `brew install poppler` (macOS) or [see here for other platforms](https://pdf2image.readthedocs.io/en/latest/installation.html)\n",
    "3. Install tesseract: `brew install tesseract` (macOS) or [see here for other platforms](https://tesseract-ocr.github.io/tessdoc/Installation.html)\n",
    "4. Download an English pipeline for spaCy: `python -m spacy download en_core_web_sm`\n",
    "\n",
    "### Configuring settings\n",
    "\n",
    "This workbook begins by loading credentials and configuration from an `.ini` file stored in `~/.hbai/file-evaluation-example.ini`. The `~` in the path refers to the current user's home directory, and the `.ini` file contents are as follows:\n",
    "\n",
    "    [openai]\n",
    "    summarize-model=gpt-3.5-turbo\n",
    "    summarize-model-max-tokens=4000\n",
    "    openai-api-key=sk-keyhere\n",
    "    azure-api-key=keyhere\n",
    "    azure-api-base=https://almitra-azure-1.openai.azure.com/\n",
    "    azure-api-version=2023-05-15\n",
    "\n",
    "    [parsing]\n",
    "    extraction-model=gpt-4-turbo\n",
    "    splitter_chunk_size=6000\n",
    "    splitter_overlap_size=500\n",
    "\n",
    "    [evaluation]\n",
    "    evaluation-model=gpt-4-turbo\n",
    "    tiktoken-model=gpt-4-turbo\n",
    "    evaluation-context=Baseline survey on consumption, savings, and financial well-being\n",
    "    evaluation-locations=United States (English), Mexico (Spanish)\n",
    "    evaluation-extra-instructions=\n",
    "    evaluation-input-path=~/Files/evaluation/example_baseline_survey.docx\n",
    "    evaluation-output-path=~/Files/evaluation\n",
    "    \n",
    "    [langsmith]\n",
    "    langsmith-api-key=optional_key_here\n",
    "\n",
    "The easiest way to get started is to take the `file-evaluation-example.ini` in the root of this repository, copy it to `~/.hbai/file-evaluation-example.ini`, and customize as follows:\n",
    "\n",
    "1. Configure for OpenAI access:\n",
    "    1. If you want to use OpenAI directly: replace `openai-api-key` with your OpenAI API key.\n",
    "    2. If you want to use Azure's OpenAI service: replace `azure-api-key` with your Azure API key, and replace `azure-api-base` and `azure-api-version` with the base URL and version of the Azure API you want to use.\n",
    "2. Set the `evaluation-context` and `evaluation-locations` with short descriptions to help the AI system understand the relevant context and setting. For each location in `evaluation-locations`, include both the location and the language, so that the system knows which languages are used in which settings.  \n",
    "3. Set the `evaluation-input-path` to the path of the survey file you want to evaluate.\n",
    "4. Set the `evaluation-output-path` to the path of the directory where you want to write the evaluation results.\n",
    "\n",
    "## Running this workbook\n",
    "\n",
    "Once you've installed dependencies and configured settings, you can run this workbook to evaluate a survey file. Run each cell in order, and follow the instructions in the output to review the raw and parsed data before continuing. All results (including intermediate outputs and log files) will be written to the `evaluation-output-path` you configured in the `.ini` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INITIALIZATION\n",
    "\n",
    "# for convenience, auto-reload modules when they've changed\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import configparser\n",
    "import os\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "# configure logging to output all messages to stdout, initialize logger\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# load credentials and other configuration from a local ini file\n",
    "inifile_location = os.path.expanduser(\"~/.hbai/file-evaluation-example.ini\")\n",
    "inifile = configparser.RawConfigParser()\n",
    "inifile.read(inifile_location)\n",
    "\n",
    "# load configuration\n",
    "openai_api_key = inifile.get(\"openai\", \"openai-api-key\")\n",
    "azure_api_key = inifile.get(\"openai\", \"azure-api-key\")\n",
    "azure_api_base = inifile.get(\"openai\", \"azure-api-base\")\n",
    "azure_api_version = inifile.get(\"openai\", \"azure-api-version\")\n",
    "if openai_api_key and not azure_api_key:\n",
    "    api_provider = \"openai\"\n",
    "else:\n",
    "    api_provider = \"azure\"\n",
    "max_combined_tokens = int(inifile.get(\"openai\", \"summarize-model-max-tokens\"))\n",
    "summarize_model = inifile.get(\"openai\", \"summarize-model\")\n",
    "parsing_model = inifile.get(\"parsing\", \"extraction-model\")\n",
    "splitter_chunk_size = int(inifile.get(\"parsing\", \"splitter_chunk_size\"))\n",
    "splitter_overlap_size = int(inifile.get(\"parsing\", \"splitter_overlap_size\"))\n",
    "gpt_model = inifile.get(\"evaluation\", \"evaluation-model\")\n",
    "tiktoken_model = inifile.get(\"evaluation\", \"tiktoken-model\")\n",
    "evaluation_context = inifile.get(\"evaluation\", \"evaluation-context\")\n",
    "if not evaluation_context:\n",
    "    evaluation_context = \"Unknown\"\n",
    "evaluation_locations = inifile.get(\"evaluation\", \"evaluation-locations\")\n",
    "if not evaluation_locations:\n",
    "    evaluation_locations = \"Unknown\"\n",
    "evaluation_extra_instructions = inifile.get(\"evaluation\", \"evaluation-extra-instructions\")\n",
    "if not evaluation_extra_instructions:\n",
    "    evaluation_extra_instructions = \"\"\n",
    "evaluation_input_path = inifile.get(\"evaluation\", \"evaluation-input-path\")\n",
    "evaluation_output_path = inifile.get(\"evaluation\", \"evaluation-output-path\")\n",
    "langsmith_api_key = inifile.get(\"langsmith\", \"langsmith-api-key\")\n",
    "\n",
    "# support LangSmith for debugging (optional)\n",
    "if langsmith_api_key:\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"survey-eval\"\n",
    "    os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "    os.environ[\"LANGCHAIN_API_KEY\"] = langsmith_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading the survey file\n",
    "\n",
    "The first step is to read the survey file to be evaluated. This step uses the `questionnaire_file_reader` module to read the survey file and extract the raw text content of each page. The raw text content is then written to `raw_data.txt` in the `evaluation-output-path` you configured in the `.ini` file. Review the raw text content to ensure that it looks reasonable before continuing.\n",
    "\n",
    "## Beware garbage-in-garbage-out\n",
    "\n",
    "Depending on the format of your survey file, the `questionnaire_file_reader` module might do a bad job reading the content. In that case, it's not worth continuing to parse and then evaluate the survey, because you'll only get garbage out. If the raw text content looks bad, you should either fix the survey file or use a different survey file.\n",
    "\n",
    "## Fixing the survey file\n",
    "\n",
    "The closer you get to a simple Word file with minimal formatting, the easier it will be for the `questionnaire_file_reader` module to read the content.\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa7fb238d10caf83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize for file reading and parsing\n",
    "from surveyeval.survey_parser import set_langchain_splits, create_schema, generate_extractor_chain, extract_data_from_file\n",
    "set_langchain_splits(splitter_chunk_size, splitter_overlap_size)\n",
    "schema, extraction_validator = create_schema()\n",
    "extractor_chain = generate_extractor_chain(parsing_model, azure_api_base, azure_api_key, azure_api_version, schema)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c96bae86b7ddbeb3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# get data from URL\n",
    "from surveyeval.survey_parser import get_data_from_url\n",
    "raw_data = get_data_from_url(os.path.expanduser(evaluation_input_path))\n",
    "# output raw data to file in output path\n",
    "raw_data_output_path = os.path.join(os.path.expanduser(evaluation_output_path), \"raw_data.txt\")\n",
    "with open(raw_data_output_path, 'w') as f:\n",
    "    for item in [doc.page_content if not isinstance(doc, str) else doc for doc in raw_data]:\n",
    "        f.write(f\"{item}\\n\")\n",
    "\n",
    "print(f\"Raw data written to {raw_data_output_path}. Review it to ensure that it looks reasonable before continuing.\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29c30724b1e5625e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Parsing the survey file\n",
    "\n",
    "The next step is to parse the raw data into a series of modules and questions. This step uses the `questionnaire_file_parser` module — as well as AI assistance — to make sense of the raw data. The parsed survey data is then written to `parsed_data.json` in the `evaluation-output-path` you configured in the `.ini` file. Review the parsed content to ensure that it looks reasonable before continuing.\n",
    "\n",
    "The parsed data should look like this:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"MODULE 1\": {\n",
    "    \"question_id_1\": [\n",
    "      {\n",
    "        \"question\": \"Question 1 in English\",\n",
    "        \"language\": \"English\",\n",
    "        \"options\": \"Options, if any\",\n",
    "        \"instructions\": \"Instructions, if any\"\n",
    "      },\n",
    "      {\n",
    "        \"question\": \"Question 1 in Spanish\",\n",
    "        \"language\": \"Spanish\",\n",
    "        \"options\": \"Options, if any\",\n",
    "        \"instructions\": \"Instructions, if any\"\n",
    "      }\n",
    "    ],\n",
    "    \"question_id_2\": [\n",
    "      {\n",
    "        \"question\": \"Question 2 in English\",\n",
    "        \"language\": \"English\",\n",
    "        \"options\": \"Options, if any\",\n",
    "        \"instructions\": \"Instructions, if any\"\n",
    "      },\n",
    "      {\n",
    "        \"question\": \"Question 2 in Spanish\",\n",
    "        \"language\": \"Spanish\",\n",
    "        \"options\": \"Options, if any\",\n",
    "        \"instructions\": \"Instructions, if any\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"MODULE 2\": {\n",
    "    \"question_id_3\": [\n",
    "      {\n",
    "        \"question\": \"Question 3 in English\",\n",
    "        \"language\": \"English\",\n",
    "        \"options\": \"Options, if any\",\n",
    "        \"instructions\": \"Instructions, if any\"\n",
    "      },\n",
    "      {\n",
    "        \"question\": \"Question 3 in Spanish\",\n",
    "        \"language\": \"Spanish\",\n",
    "        \"options\": \"Options, if any\",\n",
    "        \"instructions\": \"Instructions, if any\"\n",
    "      }\n",
    "    ],\n",
    "    \"question_id_4\": [\n",
    "      {\n",
    "        \"question\": \"Question 4 in English\",\n",
    "        \"language\": \"English\",\n",
    "        \"options\": \"Options, if any\",\n",
    "        \"instructions\": \"Instructions, if any\"\n",
    "      },\n",
    "      {\n",
    "        \"question\": \"Question 4 in Spanish\",\n",
    "        \"language\": \"Spanish\",\n",
    "        \"options\": \"Options, if any\",\n",
    "        \"instructions\": \"Instructions, if any\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## Beware garbage-in-garbage-out\n",
    "\n",
    "Depending on the format of your survey file, the `questionnaire_file_parser` module might do a bad job parsing the content. In that case, it's not worth continuing to evaluate the survey, because you'll only get garbage out. If the parsed content looks bad, you should either fix the survey file or use a different survey file. You can also try to adjust the `splitter_chunk_size` and `splitter_overlap_size` settings in the `.ini` file to see if that helps.\n",
    "\n",
    "## Fixing the survey file\n",
    "\n",
    "The closer you get to a simple Word file with minimal formatting, the easier it will be for the `questionnaire_file_parser` module to parse the content. Try to:\n",
    " \n",
    "1. Make module headings obvious\n",
    "2. Label each question with a unique number or ID\n",
    "3. Keep different translations for each question together, with the same unique number or ID  \n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa4e2838642c5462"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5eca78ea7900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARSE FILE FOR EVALUATION\n",
    "\n",
    "# output path to file we'll process\n",
    "import json\n",
    "print(evaluation_input_path)\n",
    "\n",
    "# parse file\n",
    "data = await extract_data_from_file(os.path.expanduser(evaluation_input_path), extractor_chain)\n",
    "\n",
    "# output parsed data to file so that the parsing step can itself be evaluated\n",
    "parsed_data_output_path = os.path.join(os.path.expanduser(evaluation_output_path), \"parsed_data.json\")\n",
    "with open(parsed_data_output_path, 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"Parsed data written to {parsed_data_output_path}. Review it to ensure that it looks reasonable before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Running the evaluation\n",
    "\n",
    "The final step is to run the evaluation. This step uses the `evaluation_engine` module to evaluate the parsed survey data. The evaluation results are then written to `aggregated_results.txt` in the `evaluation-output-path` you configured in the `.ini` file, along with a `logs.txt` file containing a detailed log of the evaluation process.\n",
    "\n",
    "The evaluation process has been parallelized to run more quickly, but it can still take a long time. You can monitor progress by watching the debug output in the console. If you want to stop the evaluation process, you can interrupt the kernel in your Jupyter notebook or press Ctrl+C in the console.\n",
    "\n",
    "Once you see the results, you might want to adjust the evaluation. If so, you can use the `extra_evaluation_instructions` setting in the `.ini` file to provide additional instructions to the evaluation lenses. For example:\n",
    "\n",
    "    evaluation-extra-instructions=* When making recommendations, use conversational language and phrasing \n",
    "     appropriate to informal household interviews.\n",
    "     \n",
    "     * When evaluating language and phrasing for bias, stereotypes, and prejudice, don't be more sensitive than \n",
    "     is appropriate in the survey locations considered (e.g., don't impose Western values in non-Western settings).\n",
    "\n",
    "Note that if you have more than one line of extra instructions, each line from the second one on should be indented by one space."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6696ed5810240743"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# initialize evaluation engine\n",
    "if 'evaluation_engine' in sys.modules:\n",
    "    # if it's already been loaded, force-reload to pick up any changes\n",
    "    del sys.modules['evaluation_engine']\n",
    "from surveyeval import EvaluationEngine, PhrasingEvaluationLens, ValidatedInstrumentEvaluationLens, BiasEvaluationLens, TranslationEvaluationLens\n",
    "evaluation_engine = EvaluationEngine(\n",
    "    summarize_model=summarize_model,\n",
    "    summarize_provider=api_provider,\n",
    "    evaluation_model=gpt_model,\n",
    "    evaluation_provider=api_provider,\n",
    "    openai_api_key=openai_api_key,\n",
    "    azure_api_key=azure_api_key,\n",
    "    azure_api_base=azure_api_base,\n",
    "    azure_api_version=azure_api_version,\n",
    "    temperature=0.01,\n",
    "    tiktoken_model_name=tiktoken_model,\n",
    "    max_retries=3,\n",
    "    logger=logger,\n",
    "    extra_evaluation_instructions=evaluation_extra_instructions\n",
    ")\n",
    "\n",
    "# initialize evaluation lenses\n",
    "phrasing_lens = PhrasingEvaluationLens(evaluation_engine)\n",
    "validated_instrument_lens = ValidatedInstrumentEvaluationLens(evaluation_engine)\n",
    "bias_lens = BiasEvaluationLens(evaluation_engine)\n",
    "translation_lens = TranslationEvaluationLens(evaluation_engine)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65fd55a95a8fc52f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcea3cc5b0fe1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONDUCT EVALUATION\n",
    "\n",
    "# initialize a list to store results data\n",
    "results_data = []\n",
    "\n",
    "# prepare for parallel processing of evaluation tasks\n",
    "import asyncio\n",
    "async def yield_chunked_tasks(task_list: list, chunk_size: int):\n",
    "    \"\"\"\n",
    "    Yield successive chunk_size-sized chunks from tasks.\n",
    "\n",
    "    :param task_list: List of tasks to be chunked.\n",
    "    :type task_list: list\n",
    "    :param chunk_size: Size of each chunk.\n",
    "    :type chunk_size: int\n",
    "    :return: Yields chunks of tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    for i in range(0, len(task_list), chunk_size):\n",
    "        yield task_list[i:i + chunk_size]\n",
    "\n",
    "async def process_task_chunks(task_list: list, chunk_size: int, delay: int):\n",
    "    \"\"\"\n",
    "    Process chunks of tasks with a specified delay.\n",
    "\n",
    "    :param task_list: List of tasks to process.\n",
    "    :type task_list: list\n",
    "    :param chunk_size: Size of each task chunk.\n",
    "    :type chunk_size: int\n",
    "    :param delay: Delay between processing each chunk (in seconds).\n",
    "    :type delay: int\n",
    "    \"\"\"\n",
    "\n",
    "    async def task_wrapper(task):\n",
    "        evaluation_type, text_excerpt, func = task\n",
    "        result = await func\n",
    "        return evaluation_type, text_excerpt, result\n",
    "    \n",
    "    chunk_index = 0\n",
    "    async for chunk in yield_chunked_tasks(task_list, chunk_size):\n",
    "        # process each task in the current chunk\n",
    "        wrapped_chunk = [task_wrapper(task) for task in chunk]\n",
    "        results_chunk = await asyncio.gather(*wrapped_chunk)\n",
    "        results_data.extend(results_chunk)\n",
    "        await asyncio.sleep(delay)\n",
    "        chunk_index += 1\n",
    "\n",
    "# initialize variables for tasks and results\n",
    "results = []\n",
    "tasks = []\n",
    "translations = []\n",
    "\n",
    "# run through the parsed questionnaire data and generate list of async review tasks\n",
    "k = 0\n",
    "for module in data.values():\n",
    "    # run through all questions in the module, assembling questions and translations for review\n",
    "    module_questions = []\n",
    "    full_module_text = \"\"\n",
    "    for question in module.values():\n",
    "        # consider each translation of the question\n",
    "        languages = []\n",
    "        translations = []\n",
    "        for translation in question:\n",
    "            full_question = f\"Question: {translation['question']}\\nOptions: {translation['options']}\\nInstructions: {translation['instructions']}\\n\"\n",
    "            \n",
    "            # conduct bias review on each translation independently\n",
    "            tasks.append((\"bias\", full_question, bias_lens.a_evaluate(\n",
    "                survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_question)))\n",
    "            \n",
    "            # save first language only for full phrasing and module-level review\n",
    "            if not languages:\n",
    "                module_questions.append(full_question)\n",
    "                full_module_text += full_question\n",
    "                full_module_text += \"\\n\"\n",
    "                \n",
    "            # remember this translation for translation review\n",
    "            languages.append(translation['language'])\n",
    "            translations.append(full_question)\n",
    "        \n",
    "        # conduct translation review whenever there are multiple translations\n",
    "        if len(languages) > 1:\n",
    "            primary_language_text = \"\"\n",
    "            for idx, language in enumerate(languages):\n",
    "                if not language:\n",
    "                    language = \"Unknown-language\"\n",
    "                \n",
    "                if idx == 0:\n",
    "                    primary_language_text = f\"Primary language ({language}):\\n\\n{translations[idx]}\\n\\n\"\n",
    "                else:\n",
    "                    questions_by_language = primary_language_text + f\"Translated language ({language}):\\n\\n{translations[idx]}\\n\\n\"\n",
    "                    \n",
    "                    # conduct translation review in pairwise fashion, with each translation against the first\n",
    "                    # (primary) language\n",
    "                    tasks.append((\"translation\", questions_by_language, translation_lens.a_evaluate(survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=questions_by_language)))\n",
    "\n",
    "    # execute phrasing evaluation on each module question (first language only)\n",
    "    for module_question in module_questions:\n",
    "        tasks.append((\"phrasing\", module_question, phrasing_lens.a_evaluate(survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_module_text, survey_question=module_question)))\n",
    "\n",
    "    # execute module-level evaluation lenses\n",
    "    tasks.append((\"validated_instrument\", full_module_text, validated_instrument_lens.a_evaluate(\n",
    "        survey_context=evaluation_context, survey_locations=evaluation_locations, survey_excerpt=full_module_text)))\n",
    "\n",
    "# process tasks in chunks, with a delay between chunks\n",
    "await process_task_chunks(tasks, chunk_size=20, delay=5)\n",
    "\n",
    "# output results, including a detailed processing log\n",
    "logs = []\n",
    "\n",
    "def add_log(new_history):\n",
    "    for his in new_history:\n",
    "        logs.append(\"PROMPT:\")\n",
    "        logs.append(his[0])\n",
    "        logs.append(\"RESPONSE:\")\n",
    "        logs.append(his[1])\n",
    "\n",
    "for output in results_data:\n",
    "    eval_type = output[0]\n",
    "    excerpt = output[1]\n",
    "    res, history = output[2]\n",
    "\n",
    "    if eval_type == \"phrasing\":\n",
    "        # retrieve formatted result\n",
    "        formatted_result = phrasing_lens.format_result(res)\n",
    "        eval_header = \"Phrasing evaluation:\"\n",
    "    elif eval_type == \"bias\":\n",
    "        # retrieve formatted result\n",
    "        formatted_result = bias_lens.format_result(res)\n",
    "        eval_header = \"Bias evaluation:\"\n",
    "    elif eval_type == \"translation\":\n",
    "        # retrieve formatted result\n",
    "        formatted_result = translation_lens.format_result(res)\n",
    "        eval_header = \"Translation evaluation:\"\n",
    "    elif eval_type == \"validated_instrument\":\n",
    "        # retrieve formatted result\n",
    "        formatted_result = validated_instrument_lens.format_result(res)\n",
    "        eval_header = \"Module evaluation:\"\n",
    "    else:\n",
    "        # flag unexpected evaluation types\n",
    "        eval_header = \"UNKNOWN EVALUATION\"\n",
    "        formatted_result = res\n",
    "\n",
    "    # log exchange+result\n",
    "    logs.append(eval_header)\n",
    "    add_log(history)\n",
    "    logs.append(formatted_result)\n",
    "\n",
    "    # report result\n",
    "    if formatted_result is not None and formatted_result:\n",
    "        results.append(eval_header)\n",
    "        results.append(excerpt)\n",
    "        results.append(formatted_result)\n",
    "\n",
    "# write results to file in evaluation_output_path\n",
    "\n",
    "results_output_path = os.path.join(os.path.expanduser(evaluation_output_path), \"aggregated_results.txt\")\n",
    "logs_output_path = os.path.join(os.path.expanduser(evaluation_output_path), \"logs.txt\")\n",
    "with open(results_output_path, 'w') as f:\n",
    "    f.write('\\n\\n'.join(results))\n",
    "with open(logs_output_path, 'w') as f:\n",
    "    f.write('\\n\\n'.join(logs))\n",
    "\n",
    "print(f\"Results written to {results_output_path}.\")\n",
    "print(f\"Detailed logs written to {logs_output_path}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
